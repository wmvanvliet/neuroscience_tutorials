{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6415f5-67e5-4335-83a2-b51106db7e85",
   "metadata": {},
   "source": [
    "# MNE-RSA: representational similarity analysis on sensor-level MEG data\n",
    "<img src=\"images/novice.png\" width=\"220\" style=\"float: right\">\n",
    "Ok, let's go!\n",
    "\n",
    "The dataset we will be working with today is the [Wakeman & Nelson (2015) \"faces\" dataset](https://www.nature.com/articles/sdata20151). During this experiment, participants were presented with a series of images, containing:\n",
    " - Faces of famous people that the participants likely knew\n",
    " - Faces of people that the participants likely did not know\n",
    " - Scrambled faces: the images were cut-up and randomly put together again\n",
    "\n",
    "In this tutorial, we are going to use this dataset to explore the neural representational code within the visual cortex.\n",
    "From time to time, there will be green blocks indicating it's up to you to do something, like this one:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "    \n",
    "In the cell below, update the `data_path` variable to point to where you have extracted the [`BerlinBrains2023DataPackage.zip`](https://drive.google.com/file/d/1etefiAIRG6CMBeU91Fu2CTqM5KT9Ng_Z/view?usp=drive_link) file to.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e72799-bae9-4e4c-b335-98f653f7b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 90  # Tune this to make figures bigger/smaller\n",
    "\n",
    "# Set this to where you've extracted `data.zip` to\n",
    "data_path = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2561afd3-1314-4e13-aecc-c613af6608e3",
   "metadata": {},
   "source": [
    "## A representational code for the stimuli\n",
    "\n",
    "Let's start by taking a look at the stimuli that were presented during the experiment.\n",
    "I've put them in the `stimuli` folder for you as `.bmp` image files.\n",
    "The Python Imaging Library (PIL) can open them and display them in this notebook.\n",
    "We can use the notebook's native [`IPython.display.display`](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.display) function if we want to display more than one image at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83490c2f-bdeb-41de-aa00-ecf478f71440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# Show the first \"famous\" face and the first \"scrambled\" face\n",
    "img_famous = Image.open(f\"{data_path}/stimuli/f001.bmp\")\n",
    "img_scrambled = Image.open(f\"{data_path}/stimuli/s001.bmp\")\n",
    "\n",
    "print(f\"Famous face: {img_famous.width} x {img_famous.height} pixels\")\n",
    "display(img_famous)\n",
    "print(f\"Scrambled face: {img_scrambled.width} x {img_scrambled.height} pixels\")\n",
    "display(img_scrambled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e972de76-be98-440b-8e6a-0bf019639f4d",
   "metadata": {},
   "source": [
    "Loaded like this, the stimuli are in a representational space defined by their pixels.\n",
    "Each image is represented by 128 x 162 = 20736 values between 0 (black) and 255 (white).\n",
    "Let's create a Representational Dissimilarity Matrix (RDM) where images are compared based on the difference between their pixels.\n",
    "To get the pixels of an image, you can convert it to a NumPy array like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d2d5b-cd93-4865-978b-4241f25d75ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "pixels_famous = np.array(img_famous)\n",
    "pixels_scrambled = np.array(img_scrambled)\n",
    "\n",
    "print(\"Shape of the pixel array for the famous face:\", pixels_famous.shape)\n",
    "print(\"Shape of the pixel array for the scrambled face:\", pixels_scrambled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d39fd1-aa8e-47cc-a366-80903fe9d08b",
   "metadata": {},
   "source": [
    "We can now compute the \"dissimilarity\" between the two images, based on their pixels.\n",
    "For this, we need to decide on a metric to use.\n",
    "The default metric used in the original publication ([Kiegeskorte et al. 2008](https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full)) was Pearson Correlation, so let's use that.\n",
    "Of course, correlation is a metric of similarity and we want a metric of *dis*similarity.\n",
    "Let's make it easy on ourselves and just do $1 - r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf207a-5534-4813-98b5-1013011722f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "similarity, _ = pearsonr(pixels_famous.flatten(), pixels_scrambled.flatten())\n",
    "dissimilarity = 1 - similarity\n",
    "print(f\"The dissimilarity between the pixels of the famous and scrambled faces is: {dissimilarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cd613f-ee10-48d0-9a63-1da6dbfd7abd",
   "metadata": {},
   "source": [
    "To construct the full RDM, we need to do this for all pairs of images.\n",
    "I'll talk you through the process, but I will let you do the coding for this.\n",
    "Ready? Let's go!\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "    \n",
    "In the cell below, I've already constructed a list of all image files for you.\n",
    "For first task is to load all of them (there are 450), convert them to NumPy arrays and concatenate them all together in a single big array called `pixels` of shape `n_images x width x height`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbae434c-e6b2-49ad-bccd-9a1fea089f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "files = sorted(glob(f\"{data_path}/stimuli/*.bmp\"))\n",
    "print(f\"There are {len(files)} images to read.\")\n",
    "\n",
    "pixels =  # write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c00db-335f-4d63-94c4-0778dcec7a8b",
   "metadata": {},
   "source": [
    "If you did it correctly, then executing the cell below should tell us the shape of your big array, and verify its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b8ecbd-a359-4d95-a764-01bd88507ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dimensions of the `pixel` array are:\", pixels.shape) \n",
    "if pixels.shape == (450, 162, 128):\n",
    "    print(\"These dimensions are correct! ðŸ˜Š\")\n",
    "else:\n",
    "    print(\"These dimensions are not correct. ðŸ¤”\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1549800-4549-40d3-b3da-1040022db567",
   "metadata": {},
   "source": [
    "## Your first RDM\n",
    "\n",
    "Now that you have all the images loaded in, computing the pairwise dissimilarities is a matter of looping over them and computing correlations.\n",
    "We could do this manually, but we can make our life a lot easier by using MNE-RSA's [`compute_rdm`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.compute_rdm.html) function.\n",
    "It wants the big matrix as input and also takes a `metric` parameter to select which dissimilarity metric to use.\n",
    "Setting it to `metric=\"correlation\"`, which is also the default by the way, will make it use (1 - Pearson correlation) as a metric like we did manually above.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "    \n",
    "In the cell below, I've imported the function for you.\n",
    "I'll leave it up to you to call it properly (check [its documentation](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.compute_rdm.html) if you're unsure).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728e936-ad7d-402d-aead-897a7bd3542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_rsa import compute_rdm\n",
    "pixel_rdm = # write the call to compute_dsm() here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912118e-f2e4-4158-a7fe-6d921a8d2912",
   "metadata": {},
   "source": [
    "If you did it correctly, executing the cell below will plot your RDM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717fcba6-8c25-4da0-bf1b-1140f1492921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_rsa import plot_rdms\n",
    "plot_rdms(pixel_rdm);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f71fc6-a5d3-4657-9822-f6c1776f1952",
   "metadata": {},
   "source": [
    "Staring deeply into this RDM will reveal to you which images belonged to the \"scrambled faces\" class, as those pixels are quite different from the actual faces and each other.\n",
    "We also see that for some reason, the famous faces are a little more alike than the unknown faces.\n",
    "\n",
    "The diagonal is all zeros. Take a moment to ponder why that would be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86471704-d428-47b5-9728-377b67dffb4c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>IMPLEMENTATION DETAIL</b><br>\n",
    "    The <code>compute_rdm</code> function is a wrapper around <code>scipy.spatial.distance.pdist</code>.\n",
    "    This means that all the metrics supported by <code>pdist</code> are also valid for <code>compute_dsm</code>.\n",
    "    This also means that in MNE-RSA, the native format for an RDM is the so-called \"condensed\" form.\n",
    "    Since RDMs are symmetric, only the upper triangle is stored.\n",
    "    The <code>scipy.spatial.distance.squareform</code> function can be used to go from a square matrix to its condensed form and back.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f5d3a3-e405-4b67-afc2-df27e03626c5",
   "metadata": {},
   "source": [
    "## Your second RDM\n",
    "\n",
    "There are many sensible representations possible for images.\n",
    "One intriguing one is to create them using convolutional neural networks (CNNs).\n",
    "For example, there is the [FaceNet](https://github.com/davidsandberg/facenet) model by [Schroff et al. (2015)](http://arxiv.org/abs/1503.03832) that can generate high-level representations, such that different photos of the same face have similar representations.\n",
    "I have run the stimulus images through FaceNet and recorded the generated embeddings for you to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b745df5-d5dc-499f-9724-4c486568628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = np.load(f\"{data_path}/stimuli/facenet_embeddings.npz\")\n",
    "filenames = store[\"filenames\"]\n",
    "embeddings = store[\"embeddings\"]\n",
    "print(f\"For each of the 450 images, the embedding is a vector of length 512: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286c0e7-517c-4a2c-9fcc-f9bff07bf0a6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "    \n",
    "I leave it up to you to construct the RDM based on the FaceNet embedding vectors using the [`compute_rdm`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.compute_rdm.html) function.\n",
    "Use Pearson correlation as dissimility metric and store the RDM in a variable called `facenet_rdm`.\n",
    "Make sure that the stimuli are in the same order as the pixel RDM we created earlier!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d33f777-c2d0-4b81-9793-60a4cb9df884",
   "metadata": {},
   "outputs": [],
   "source": [
    "facenet_rdm = # write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f3e6c-c2a9-4e99-bcbe-609a6a32919b",
   "metadata": {},
   "source": [
    "If you created the FaceNet RDM correctly, executing the cell below should plot both RDMs side-by-side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2886c2-0e27-4555-826a-ef9cc8997dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rdms([pixel_rdm, facenet_rdm], names=[\"pixels\", \"facenet\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d811553-b0c0-42f0-bd55-a1ed370e9009",
   "metadata": {},
   "source": [
    "## A look at the brain data\n",
    "\n",
    "We've seen how we can create RDMs using properties of the images or embeddings generated by a model.\n",
    "Now it's time to see how we create RDMs based on the MEG data.\n",
    "For that, we first load the epochs from a single participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14358f7-6f47-4032-8164-1d0a438c0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "epochs = mne.read_epochs(f\"{data_path}/sub-02/sub-02-epo.fif\")\n",
    "epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026759bf-b8b6-47da-b88c-126096d42e0b",
   "metadata": {},
   "source": [
    "Each epoch corresponds to the presentation of an image, and the signal across the sensors over time can be used as the neural representation of that image.\n",
    "Hence, one could make a neural RDM, of for example the gradiometers, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb2f7a-504b-4cd8-99a0-921426669d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_rdm = compute_rdm(epochs.copy().pick(\"grad\").crop(0.1, 0.2).get_data())\n",
    "plot_rdms(neural_rdm);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db1fe4-828b-455b-af03-97a6d281fa17",
   "metadata": {},
   "source": [
    "To compute RSA scores, we want to compare the resulting neural RDM with the RDMs we've created earlier.\n",
    "However, if we inspect the neural RDM closely, we see that its rows and column don't line up with those of the previous RDMs.\n",
    "There are too many (879 vs. 450) and they are in the wrong order. Making sure that the RDMs match is an important and sometimes tricky part of RSA.\n",
    "\n",
    "To help us out, a useful feature of MNE-Python is that epochs have an associated [`epochs.metadata`](https://mne.tools/stable/auto_tutorials/epochs/30_epochs_metadata.html) field.\n",
    "This metadata is a [Pandas DataFrame](https://pandas.pydata.org/docs/user_guide/dsintro.html#dataframe) where each row contains information about the corresponding epoch.\n",
    "The epochs in this tutorial come with some useful `.metadata` already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e8d42-39f3-46e4-83c2-4da33008b7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efd1716-631f-421c-a911-fb20c9273b4f",
   "metadata": {},
   "source": [
    "While the trigger codes only indicate what type of stimulus was shown, the `file` column of the metadata tells us the exact image.\n",
    "Couple of challenges here: the stimuli where shown in a random order, stimuli were repeated twice during the experiment, and some epochs were dropped during preprocessing so not every image is necessarily present twice in the `epochs` data. ðŸ˜©\n",
    "\n",
    "Luckily, MNE-RSA has a way to make our lives easier.\n",
    "Let's take a look at the [`rdm_epochs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rdm_epochs.html) function, the Swiss army knife for computing RDMs from an MNE-Python `epochs` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8452c77-33e8-4e5f-b0d3-54fb79ce005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_rsa import rdm_epochs\n",
    "rdm_epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401134f9-5dee-48a4-8c95-86aea9509297",
   "metadata": {},
   "source": [
    "In MNE-Python tradition, the function has a lot of parameters, but all-but-one have a default so you only have to specify the ones that are relevant to you.\n",
    "For example, to redo the neural RDM we created above, we could do something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ff430-9744-4831-a892-160cb3f1af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_rdm_gen = rdm_epochs(epochs, tmin=0.1, tmax=0.2)\n",
    "\n",
    "# dsm_epochs returns a generator of RDMs\n",
    "# unpacking the first (and only) RDM from the generator\n",
    "neural_rdm = next(neural_rdm_gen)\n",
    "plot_rdms(neural_rdm);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf11484-5a2a-4238-bd9e-84ee5ced89fc",
   "metadata": {},
   "source": [
    "Take note that [`rdm_epochs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rdm_epochs.html) returns a [generator](https://wiki.python.org/moin/Generators) of RDMs.\n",
    "This is because one of the main use-cases for MNE-RSA is to produce RDMs using sliding windows (in time and also in space), which can produce a large amount of RDMs that can take up a lot of memory of you're not careful.\n",
    "\n",
    "## The `y` parameter that solves ~~all~~ most alignment problems\n",
    "Looking at the neural RDM above, something is clearly different from the one we made before.\n",
    "This one has 9 rows and columns.\n",
    "Closely inspecting the docstring of [`rdm_epochs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rdm_epochs.html) reveals that it is the `y` parameter that is responsible for this:\n",
    "\n",
    "```\n",
    "y : ndarray of int, shape (n_items,) | None\n",
    "    For each Epoch, a number indicating the item to which it belongs. When\n",
    "    None, the event codes are used to differentiate between items.\n",
    "    Defaults to None.\n",
    "```\n",
    "\n",
    "Instead of producing one row per epoch, [`rdm_epochs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rdm_epochs.html) produced one row per event type, averaging across epochs of the same type before computing dissimilarity.\n",
    "This is not quite what we want though.\n",
    "If we want to match `pixel_rdm` and `facenet_rdm`, we want every single one of the 450 images to be its own stimulus type.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "    \n",
    "Turning it over to you: in the cell below, write the code necessary to construct the desired neural RDM.\n",
    "This is your first real challenge in this workshop.\n",
    "Keep the following in mind:\n",
    "\n",
    " - Each of the 450 images should be on a row by itself\n",
    " - We will achieve this by setting the `y` parameter of [`rdm_epochs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rdm_epochs.html) to a list that assigns each of the 879 epochs to a number from 1-450 (or 0-449) indicating which image was shown. Take care to assign number according to the order in which they appear in `pixel_rdm` and `facenet_rdm`.\n",
    " - An image is identified by its filename, and we have the `files` and `filenames` variables left over from earlier that contain all the images in the proper order.\n",
    " - The `epochs.metadata[\"file\"]` column contains the filenames corresponding to the epochs.\n",
    " - Tell [`rdm_epochs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rdm_epochs.html) to only consider data from 0.1 to 0.2 seconds.\n",
    " - The result will be a generator. Use `next()` to unpack the RDM from it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1672d1-72cb-4c34-a6fa-f2790d173fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = # compute y here\n",
    "neural_rdm = # compute the RDM here\n",
    "\n",
    "# This plots your RDM\n",
    "plot_rdms(neural_rdm);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912423dd-7e29-4d8c-a176-fb3272919b7d",
   "metadata": {},
   "source": [
    "If you've done it correctly, the cell below will compure RSA between the neural RDM and the pixel and FaceNet RDMs we created earlier.\n",
    "The RSA score will be the Spearman correlation between the RDMs, which is the default metric used in the [original RSA paper](https://www.frontiersin.org/articles/10.3389/neuro.06.004.2008/full)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840fc74e-2803-4fbf-a477-f0b32c3885b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_rsa import rsa\n",
    "rsa_pixel = rsa(neural_rdm, pixel_rdm, metric=\"spearman\")\n",
    "rsa_facenet = rsa(neural_rdm, facenet_rdm, metric=\"spearman\")\n",
    "\n",
    "print(\"RSA score between neural RDM and pixel RDM:\", rsa_pixel)\n",
    "print(\"RSA score between neural RDM and FaceNet RDM:\", rsa_facenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71d57f-e011-4afa-9b7d-6c6e5da3e7fe",
   "metadata": {},
   "source": [
    "## Slippin' and slidin' across time\n",
    "\n",
    "The neural representation of a stimulus is different across brain regions and evolves over time.\n",
    "For example, we would expect that the pixel RDM would be more similar to a neural RDM that we computed across the visual cortex at an early time point, and that the FaceNET RDM might be more similar to a neural RDM that we computed at a later time point.\n",
    "\n",
    "For the remainder of this notebook, we'll restrict the `epochs` to only contain the sensors over the left occipital cortex.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>IMPORTANT NOTE</b><br>\n",
    "    Just because we select sensors over a certain brain region, does not mean the magnetic fields originate from that region.\n",
    "    This is especially true for magnetometers. To make it a bit more accurate, we only select gradiometers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f753bc22-826e-4586-98af-55c804b4218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "picks = mne.channels.read_vectorview_selection(\"Left-occipital\")\n",
    "picks = [\"\".join(p.split(\" \")) for p in picks]\n",
    "epochs.pick(picks).pick(\"grad\").crop(-0.1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e9d13-f266-49d3-b75d-b286eaafc67d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "    \n",
    "In the cell below, use [`rdm_epochs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rdm_epochs.html) to compute RDMs using a sliding window by setting the `temporal_radius` parameter to `0.1` seconds.\n",
    "Use the entire time range (`tmin=None` and `tmax=None`) and leave the result as a generator (so no `next()` calls).\n",
    "Store the resulting generator in a variable called `neural_dsms_gen`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aa6b81-3515-4c6a-8d12-ad7593ea6131",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_rdms_gen = # write your call to rdm_epochs() here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87567d32-1bd9-4692-928f-7ddd5af6504c",
   "metadata": {},
   "source": [
    "If you did it correctly, the cell below will consume the generator (with a nice progress bar) and plot a few of the generated RDMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4fce9d-0b6c-4777-b543-11e06b19b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "times = epochs.times[(epochs.times >= 0) & (epochs.times <= 0.9)]\n",
    "neural_rdms_list = list(tqdm(neural_rdms_gen, total=len(times)))\n",
    "plot_rdms(neural_rdms_list[::10], names=[f\"t={t:.2f}\" for t in times[::10]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5b1ec-3b73-4639-acbc-061c5fea3eda",
   "metadata": {},
   "source": [
    "## Putting it altogether for sensor-level RSA\n",
    "\n",
    "Now all that is left to do is compute RSA scored between the neural RDMs you've just created and the pixel and FaceNet RDMs.\n",
    "We could do this using the [`rsa_gen`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rsa_gen.html) function, but I'd rather directly show you the [`rsa_epochs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rsa_epochs.htm) function that combines computing the neural RDMs with computing the RSA scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd2456-cefd-4b4d-a04e-fad5bea9d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne_rsa import rsa_epochs\n",
    "rsa_epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b150008-0cbc-4c3f-81fd-e8ad4bc263de",
   "metadata": {},
   "source": [
    "The signature of [`rsa_epochs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rsa_epochs.htm) is very similar to that of [`dsm_epochs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.dsm_epochs.html).\n",
    "The main difference is that we also give it the \"model\" RDMs, in our case the pixel and FaceNet RDMs.\n",
    "[`rsa_epochs`](https://users.aalto.fi/~vanvlm1/mne-rsa/functions/mne_rsa.rsa_epochs.htm) will return the RSA scores as a list of `mne.Evoked` objects: one for each model RDM we gave it.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE</b>:\n",
    "    \n",
    "Go ahead and:\n",
    " - compute the RSA scores for `epochs` gainst `[pixel_rdm, facenet_rdm]`\n",
    " - do this in a sliding windows across time, with a temporal radius of 0.1 seconds\n",
    " - optionally set `verbose=True` to activate a progress bar\n",
    " - optionally set `n_jobs=-1` to use multiple CPU cores to speed things up\n",
    " - store the result in a variable called `ev_rsa`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ace896-6e73-4489-8fcb-5b583776e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_rsa = # write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716b28ba-b8cb-4c20-8435-f72491592b52",
   "metadata": {},
   "source": [
    "If you did it correctly, executing the cell below will create a nice plot of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd18e0-8d32-4283-9e54-297d092a08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_rsa[0].comment = \"pixels\"\n",
    "ev_rsa[1].comment = \"facenet\"\n",
    "mne.viz.plot_compare_evokeds(ev_rsa, picks=[0], ylim=dict(misc=[-0.02, 0.2]), show_sensors=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e5e97-187c-46b6-a54d-c1978efb4822",
   "metadata": {},
   "source": [
    "If you've made it this far, you have successfully completed your first sensor-level RSA! ðŸŽ‰\n",
    "This is the end of this notebook.\n",
    "I invite you to join me in the [next notebook](fast_source_level.ipynb) where we will do source level RSA.\n",
    "\n",
    "<center>\n",
    "<a href=\"source_level.ipynb\"><img width=\"200\" src=\"images/adept.png\"></a><br>\n",
    "<a href=\"source_level.ipynb\" style=\"font-size: 20pt\">>>>>> Continue to source-level RSA >>>>></a>\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
