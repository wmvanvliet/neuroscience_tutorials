
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>tutorial3</title><meta name="generator" content="MATLAB 7.14"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2013-01-16"><meta name="DC.source" content="tutorial3.m"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, tt, code { font-size:12px; }
pre { margin:0px 0px 20px; }
pre.error { color:red; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#2">3. Imagined movement</a></li><li><a href="#3">Credits</a></li><li><a href="#4">Obtaining the data</a></li><li><a href="#8">Plotting the data</a></li><li><a href="#13">Classifying the data</a></li></ul></div><pre class="codeinput"><span class="keyword">function</span> tutorial3()
</pre><h2>3. Imagined movement<a name="2"></a></h2><p>In this tutorial we will look at imagined movement. Our movement is controlled in the motor cortex where there is an increased level of mu activity (8&#8211;12 Hz) when we perform movements. This is accompanied by a reduction of this mu activity in specific regions that deal with the limb that is currently moving. This decrease is called Event Related Desynchronization (ERD). By measuring the amount of mu activity at different locations on the motor cortex, we can determine which limb the subject is moving. Through mirror neurons, this effect also occurs when the subject is not actually moving his limbs, but merely imagining it.</p><h2>Credits<a name="3"></a></h2><p>The CSP code was originally written by Boris Reuderink of the Donders Institute for Brain, Cognition and Behavior. It is part of his Python EEG toolbox: https://github.com/breuderink/eegtools</p><p>Inspiration for this tutorial also came from the excellent code example given in the book chapter:</p><p>Arnaud Delorme, Christian Kothe, Andrey Vankov, Nima Bigdely-Shamlo, Robert Oostenveld, Thorsten Zander, and Scott Makeig. MATLAB-Based Tools for BCI Research, In <i>(B+H)CI: The Human in Brain-Computer Interfaces and the Brain in Human-Computer Interaction.</i> Desney S. Tan and Anton Nijholt (eds.), 2009, 241-259, <a href="http://dx.doi.org/10.1007/978-1-84996-272-8">http://dx.doi.org/10.1007/978-1-84996-272-8</a></p><h2>Obtaining the data<a name="4"></a></h2><p>The dataset for this tutorial is provided by the fourth BCI competition, which you will have to download youself. First, go to <a href="http://www.bbci.de/competition/iv/#download">http://www.bbci.de/competition/iv/#download</a> and fill in your name and email address. An email will be sent to you automatically containing a username and password for the download area.</p><p>Download Data Set 1, from Berlin, the 100Hz version in MATLAB format: <a href="http://bbci.de/competition/download/competition_iv/BCICIV_1_mat.zip">http://bbci.de/competition/download/competition_iv/BCICIV_1_mat.zip</a> and unzip it in a subdirectory called 'data_set_IV'. This subdirectory should be inside the directory in which you've stored the tutorial files.</p><p>Also download the true labels of this dataset: <a href="http://bbci.de/competition/iii/results/berlin_IVa/true_labels_al.mat">http://bbci.de/competition/iii/results/berlin_IVa/true_labels_al.mat</a> and store them in the same 'data_set_IV' subdirectory you created earlier.</p><p><a href="http://bbci.de/competition/iv/desc_1.html">Description of the data</a></p><p>If you've followed the instructions above, the following code should load the data:</p><pre class="codeinput">m = load(<span class="string">'data_set_IV/100Hz/data_set_IVa_al.mat'</span>);
m2 = load(<span class="string">'data_set_IV/true_labels_al.mat'</span>);

sample_rate = m.nfo.fs;
EEG = m.cnt';
nchannels = size(EEG, 1);
nsamples = size(EEG, 2);

channel_names = m.nfo.clab;
event_onsets = m.mrk.pos;
event_codes = m2.true_y;
cl_lab = m.mrk.className;
nclasses = length(cl_lab);
nevents = length(event_onsets);
</pre><p>Now we have the data in the following variables:</p><pre class="codeinput"><span class="comment">% Print some information</span>
disp(<span class="string">'Shape of EEG:'</span>); disp(size(EEG));
disp(<span class="string">'Sample rate:'</span>); disp(sample_rate);
disp(<span class="string">'Number of channels:'</span>); disp(nchannels);
disp(<span class="string">'Channel names:'</span>); disp(channel_names);
disp(<span class="string">'Number of events:'</span>); disp(length(event_onsets));
disp(<span class="string">'Event codes:'</span>); disp(unique(event_codes));
disp(<span class="string">'Class labels:'</span>); disp(cl_lab);
disp(<span class="string">'Number of classes:'</span>); disp(nclasses);
</pre><pre class="codeoutput">Shape of EEG:
         118      283574

Sample rate:
   100

Number of channels:
   118

Channel names:
  Columns 1 through 8

    'Fp1'    'AFp1'    'Fpz'    'AFp2'    'Fp2'    'AF7'    'AF3'    'AF4'

  Columns 9 through 16

    'AF8'    'FAF5'    'FAF1'    'FAF2'    'FAF6'    'F7'    'F5'    'F3'

  Columns 17 through 25

    'F1'    'Fz'    'F2'    'F4'    'F6'    'F8'    'FFC7'    'FFC5'    'FFC3'

  Columns 26 through 33

    'FFC1'    'FFC2'    'FFC4'    'FFC6'    'FFC8'    'FT9'    'FT7'    'FC5'

  Columns 34 through 41

    'FC3'    'FC1'    'FCz'    'FC2'    'FC4'    'FC6'    'FT8'    'FT10'

  Columns 42 through 48

    'CFC7'    'CFC5'    'CFC3'    'CFC1'    'CFC2'    'CFC4'    'CFC6'

  Columns 49 through 57

    'CFC8'    'T7'    'C5'    'C3'    'C1'    'Cz'    'C2'    'C4'    'C6'

  Columns 58 through 65

    'T8'    'CCP7'    'CCP5'    'CCP3'    'CCP1'    'CCP2'    'CCP4'    'CCP6'

  Columns 66 through 73

    'CCP8'    'TP9'    'TP7'    'CP5'    'CP3'    'CP1'    'CPz'    'CP2'

  Columns 74 through 81

    'CP4'    'CP6'    'TP8'    'TP10'    'PCP7'    'PCP5'    'PCP3'    'PCP1'

  Columns 82 through 89

    'PCP2'    'PCP4'    'PCP6'    'PCP8'    'P9'    'P7'    'P5'    'P3'

  Columns 90 through 98

    'P1'    'Pz'    'P2'    'P4'    'P6'    'P8'    'P10'    'PPO7'    'PPO5'

  Columns 99 through 106

    'PPO1'    'PPO2'    'PPO6'    'PPO8'    'PO7'    'PO3'    'PO1'    'POz'

  Columns 107 through 114

    'PO2'    'PO4'    'PO8'    'OPO1'    'OPO2'    'O1'    'Oz'    'O2'

  Columns 115 through 118

    'OI1'    'OI2'    'I1'    'I2'

Number of events:
   280

Event codes:
     1     2

Class labels:
    'right'    'foot'

Number of classes:
     2

</pre><p>This is a large recording: 118 electrodes where used, spread across the entire scalp. The subject was given a cue and then imagined either right hand movement or the movement of his feet. As can be seen from the <a href="http://en.wikipedia.org/wiki/Cortical_homunculus">Homunculus</a>, foot movement is controlled at the center of the motor cortex (which makes it hard to distinguish left from right foot), while hand movement is controlled more lateral.</p><p>The code below cuts trials for the two classes and should look familiar if you've completed the previous tutorials. Trials are cut in the interval [0.5&#8211;2.5 s] after the onset of the cue.</p><pre class="codeinput"><span class="comment">% Struct to store the trials in, each class gets an entry</span>
trials = struct();

<span class="comment">% The time window (in samples) to extract for each trial, here 0.5 -- 2.5</span>
<span class="comment">% seconds</span>
win = fix(0.5*sample_rate):fix(2.5*sample_rate)-1;

<span class="comment">% Length of the time window</span>
nsamples = length(win);

<span class="comment">% Loop over the classes (right, foot)</span>
codes = unique(event_codes);
<span class="keyword">for</span> i = 1:length(cl_lab)
    cl = cl_lab{i};
    code = codes(i);

    <span class="comment">% Extract the onsets for the class</span>
    cl_onsets = event_onsets(event_codes == code);

    <span class="comment">% Allocate memory for the trials</span>
    trials.(cl) = zeros(nchannels, nsamples, length(cl_onsets));

    <span class="comment">% Extract each trial</span>
    <span class="keyword">for</span> j = 1:length(cl_onsets)
        onset = cl_onsets(j);
        trials.(cl)(:,:,j) = EEG(:, win+onset);
    <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="comment">% Some information about the dimensionality of the data (channels x time x trials)</span>
disp(<span class="string">'Shape of trials.right:'</span>); disp(size(trials.right));
disp(<span class="string">'Shape of trials.foot:'</span>); disp(size(trials.foot));
</pre><pre class="codeoutput">Shape of trials.right:
   118   200   140

Shape of trials.foot:
   118   200   140

</pre><h2>Plotting the data<a name="8"></a></h2><p>Since the feature we're looking for (a decrease in mu activity) is a frequency feature, lets plot the PSD of the trials in a similar manner as with the SSVEP data. The code below defines a function that computes the PSD for each trial (we're going to need it again later on):</p><pre class="codeinput"><span class="keyword">function</span> [trials_PSD, freqs] = calc_psd(trials)
<span class="comment">%     Calculates for each trial the Power Spectral Density (PSD).</span>
<span class="comment">%     arguments:</span>
<span class="comment">%         trials - An array (channels x samples x trials) containing the signal.</span>
<span class="comment">%     returns:</span>
<span class="comment">%         An array (channels x PSD x trials) containing the PSD for each trial.</span>
<span class="comment">%         A list containing the frequencies for which the PSD was computed (useful for plotting later)</span>

    ntrials = size(trials, 3);
    trials_PSD = zeros(nchannels, 101, ntrials);

    <span class="comment">% Iterate over trials and channels</span>
    <span class="keyword">for</span> trial = 1:ntrials
        <span class="keyword">for</span> ch = 1:nchannels
            <span class="comment">% Calculate the PSD</span>
            [PSD, freqs] = pwelch(squeeze(trials(ch,:,trial)), nsamples, 0, nsamples, sample_rate);
            trials_PSD(ch, :, trial) = PSD;
        <span class="keyword">end</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeinput"><span class="comment">% Apply the function</span>
[psd_r, freqs] = calc_psd(trials.right);
[psd_f, freqs] = calc_psd(trials.foot);
trials_PSD = struct(<span class="string">'right'</span>, psd_r, <span class="string">'foot'</span>, psd_f);
</pre><p>The function below plots the PSDs that are calculated with the above function. Since plotting it for 118 channels will clutter the display, it takes the indices of the desired channels as input, as well as some metadata to decorate the plot.</p><pre class="codeinput"><span class="keyword">function</span> plot_psds(trials_PSD, freqs, chan_ind, chan_lab, maxy)
<span class="comment">%     Plots PSD data calculated with psd().</span>
<span class="comment">%     arguments:</span>
<span class="comment">%         trials   - The PSD data, as returned by psd()</span>
<span class="comment">%         freqs    - The frequencies for which the PSD is defined, as returned by psd()</span>
<span class="comment">%         chan_ind - List of indices of the channels to plot</span>
<span class="comment">%         chan_lab - (optional) List of names for each channel</span>
<span class="comment">%         maxy     - (optional) Limit the y-axis to this value</span>
    figure(<span class="string">'Position'</span>, [100, 100, 1000, 300]);

    nchans = length(chan_ind);

    <span class="comment">% Maximum of 3 plots per row</span>
    nrows = ceil(nchans / 3);
    ncols = min(3, nchans);

    <span class="comment">% Enumerate over the channels</span>
    <span class="keyword">for</span> i = 1:length(chan_ind)
        ch = chan_ind(i);

        <span class="comment">% Figure out which subplot to draw to</span>
        subplot(nrows,ncols,i);

        <span class="comment">% Plot the PSD for each class</span>
        hold <span class="string">on</span>;
        colors = {<span class="string">'b'</span>, <span class="string">'r'</span>};
        <span class="keyword">for</span> j = 1:nclasses
            cl = cl_lab{j};
            plot(freqs, squeeze(mean(trials_PSD.(cl)(ch,:,:), 3)), colors{j});
        <span class="keyword">end</span>
        hold <span class="string">off</span>;

        <span class="comment">% All plot decoration below...</span>
        xlim([1, 30]);
        ylim([0, maxy]);

        grid()

        xlabel(<span class="string">'Frequency (Hz)'</span>)
        title(chan_lab{i});
        legend(cl_lab);
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><p>Lets put the `plot_psds()` function to use and plot three channels:</p><div><ol><li>C3: Central, left</li><li>Cz: Central, central</li><li>C4: Central, right</li></ol></div><pre class="codeinput">channels_of_interest = [find(strcmp(channel_names, <span class="string">'C3'</span>)), <span class="keyword">...</span>
                        find(strcmp(channel_names, <span class="string">'Cz'</span>)), <span class="keyword">...</span>
                        find(strcmp(channel_names, <span class="string">'C4'</span>))];
plot_psds(trials_PSD, freqs, channels_of_interest, <span class="keyword">...</span>
         {<span class="string">'left'</span>, <span class="string">'center'</span>, <span class="string">'right'</span>}, 2500);
</pre><img vspace="5" hspace="5" src="tutorial3_01.png" alt=""> <p>A spike of mu activity can be seen on each channel for both classes. At the left hemisphere, the mu for the right hand movement is lower than for the foot movement due to the ERD. At the central electrode, the mu for the foot movement is reduced and at the right electrode the mu activity is about equal for both classes. This is in line with the theory that the right hand is controlled by the left hemiphere and the feet are controlled centrally.</p><h2>Classifying the data<a name="13"></a></h2><p>We will use a machine learning algorithm to construct a model that can distinguish between the right hand and foot movement of this subject. In order to do this we need to:</p><div><ol><li>find a way to quantify the amount of mu activity present in a trial</li><li>make a model that describes expected values of mu activity for each class</li><li>finally test this model on some unseen data to see if it can predict the correct class label</li></ol></div><p>We will follow a classic BCI design by Blankertz et al. [1] where they use the logarithm of the variance of the signal in a certain frequency band as a feature for the classifier.</p><p>[1] Blankertz, B., Dornhege, G., Krauledat, M., M&uuml;ller, K.-R., &amp; Curio, G. (2007). The non-invasive Berlin Brain-Computer Interface: fast acquisition of effective performance in untrained subjects. <i>NeuroImage</i>, 37(2), 539&#8211;550. doi:10.1016/j.neuroimage.2007.01.051</p><p>The script below designs a band pass filter using the <tt>butter</tt> function that will strip away frequencies outside the 8--15Hz window. The filter is applied to all trials:</p><pre class="codeinput"><span class="keyword">function</span> trials_filt = bandpass(trials, lo, hi, sample_rate)
<span class="comment">%     Designs and applies a bandpass filter to the signal.</span>
<span class="comment">%     arguments:</span>
<span class="comment">%         trials      - An array (channels x samples x trials) containing the signal</span>
<span class="comment">%         lo          - Lower frequency bound (in Hz)</span>
<span class="comment">%         hi          - Upper frequency bound (in Hz)</span>
<span class="comment">%         sample_rate - Sample rate of the signal (in Hz)</span>
<span class="comment">%     returns:</span>
<span class="comment">%         An array (channels x samples x trials) containing the bandpassed</span>
<span class="comment">%         signal</span>
    ntrials = size(trials, 3);

    <span class="comment">% The butter() function takes the filter order: higher numbers mean a</span>
    <span class="comment">% sharper frequency cutoff, but the resulting signal might be shifted</span>
    <span class="comment">% in time, lower numbers mean a soft frequency cutoff, but the</span>
    <span class="comment">% resulting signal less distorted in time. It also takes the lower and</span>
    <span class="comment">% upper frequency bounds to pass, divided by the niquist frequency,</span>
    <span class="comment">% which is the sample rate divided by 2:</span>
    [a, b] = butter(3, [lo/(sample_rate/2.0), hi/(sample_rate/2.0)]);

    <span class="comment">% Applying the filter to each trial.</span>
    <span class="comment">% filtfilt operates across the first non-singleton dimension, so we</span>
    <span class="comment">% permute the trials array so that the samples are on the first</span>
    <span class="comment">% dimension</span>
    trials = permute(trials, [2, 1, 3]);

    trials_filt = zeros(size(trials));
    <span class="keyword">for</span> i = 1:ntrials
        trials_filt(:,:,i) = filtfilt(a, b, trials(:,:,i));
    <span class="keyword">end</span>

    <span class="comment">% Undo the permutation performed earlier</span>
    trials_filt = permute(trials_filt, [2, 1, 3]);
<span class="keyword">end</span>
</pre><p>Apply the function:</p><pre class="codeinput">trials_filt = struct(<span class="string">'right'</span>, bandpass(trials.right, 8, 15, sample_rate), <span class="keyword">...</span>
                     <span class="string">'foot'</span>, bandpass(trials.foot, 8, 15, sample_rate));
</pre><p>Plotting the PSD of the resulting trials_filt shows the suppression of frequencies outside the passband of the filter:</p><pre class="codeinput">[psd_r, freqs] = calc_psd(trials_filt.right);
[psd_f, freqs] = calc_psd(trials_filt.foot);
trials_PSD = struct(<span class="string">'right'</span>, psd_r, <span class="string">'foot'</span>, psd_f);

plot_psds(trials_PSD, freqs, channels_of_interest, <span class="keyword">...</span>
          {<span class="string">'left'</span>, <span class="string">'center'</span>, <span class="string">'right'</span>}, 3000);
</pre><img vspace="5" hspace="5" src="tutorial3_02.png" alt=""> <p>As a feature for the classifier, we will use the logarithm of the variance of each channel. The function below calculates this:</p><pre class="codeinput"><span class="keyword">function</span> x = logvar(trials)
<span class="comment">%     Calculate the log-var of each channel.</span>
<span class="comment">%     arguments:</span>
<span class="comment">%         trials - An array (channels x samples x trials) containing the signals.</span>
<span class="comment">%     returns:</span>
<span class="comment">%         An array (channels x trials) containing for each channel the logvar of the signal</span>

    <span class="comment">% var operates along the first non singleton dimension, permute the</span>
    <span class="comment">% array to align the samples along the first dimension</span>
    trials = permute(trials, [2, 1, 3]);

    <span class="comment">% Calculate the log-var</span>
    x = log(squeeze(var(trials)));
<span class="keyword">end</span>
</pre><p>Apply the function</p><pre class="codeinput">trials_logvar = struct(<span class="string">'right'</span>, logvar(trials_filt.right), <span class="keyword">...</span>
                       <span class="string">'foot'</span>, logvar(trials_filt.foot));
</pre><p>Below is a function to visualize the logvar of each channel as a bar chart:</p><pre class="codeinput"><span class="keyword">function</span> plot_logvar(trials)
<span class="comment">%     Plots the log-var of each channel/component.</span>
<span class="comment">%     arguments:</span>
<span class="comment">%         trials - Dictionary containing the trials (log-vars x trials) for 2 classes.</span>
    figure(<span class="string">'Position'</span>, [100, 100, 1200, 400]);

    x0 = (1:nchannels);
    x1 = (1:nchannels) + 0.4;

    y0 = mean(trials.right, 2);
    y1 = mean(trials.foot, 2);

    hold <span class="string">on</span>;
    bar(x0, y0, 0.5, <span class="string">'b'</span>);
    bar(x1, y1, 0.4, <span class="string">'r'</span>);
    hold <span class="string">off</span>;

    xlim([-0.5, nchannels+0.5]);

    grid();
    title(<span class="string">'log-var of each channel/component'</span>);
    xlabel(<span class="string">'channels/components'</span>);
    ylabel(<span class="string">'log-var'</span>);
    legend({<span class="string">'right'</span>, <span class="string">'foot'</span>});
<span class="keyword">end</span>
</pre><p>Plot the log-vars</p><pre class="codeinput">plot_logvar(trials_logvar);
</pre><img vspace="5" hspace="5" src="tutorial3_03.png" alt=""> <p>We see that most channels show a small difference in the log-var of the signal between the two classes. The next step is to go from 118 channels to only a few channel mixtures. The CSP algorithm calculates mixtures of channels that are designed to maximize the difference in variation between two classes. These mixures are called spatial filters.</p><pre class="codeinput"><span class="keyword">function</span> W = csp(trials_r, trials_f)
<span class="comment">%     Calculate the CSP transformation matrix W.</span>
<span class="comment">%     arguments:</span>
<span class="comment">%         trials_r - Array (channels x samples x trials) containing right hand movement trials</span>
<span class="comment">%         trials_f - Array (channels x samples x trials) containing foot movement trials</span>
<span class="comment">%     returns:</span>
<span class="comment">%         Mixing matrix W</span>
    cov_r = trial_cov(trials_r);
    cov_f = trial_cov(trials_f);
    P = whitening(cov_r + cov_f);
    [B,~,~] = svd(P' * cov_f * P);
    W = P * B;
<span class="keyword">end</span>

<span class="keyword">function</span> X = trial_cov(trials)
    <span class="comment">% Calculate the covariance for each trial and return their average</span>
    [nchannels, nsamples, ntrials] = size(trials);
    covs = zeros(nchannels, nchannels, ntrials);
    <span class="keyword">for</span> i = 1:ntrials
        covs(:,:,i) = (squeeze(trials(:,:,i)) * squeeze(trials(:,:,i))') / nsamples;
    <span class="keyword">end</span>

    X = mean(covs, 3);
<span class="keyword">end</span>

<span class="keyword">function</span> X = whitening(sigma)
    <span class="comment">% Calculate a whitening matrix for covariance matrix sigma.</span>
    [U, l, ~] = svd(sigma);
    X = U * (l ^ -0.5);
<span class="keyword">end</span>

<span class="keyword">function</span> trials_csp = apply_mix(W, trials)
    <span class="comment">% Apply a mixing matrix to each trial (basically multiply W with the</span>
    <span class="comment">% EEG signal matrix)</span>
    ntrials = size(trials, 3);
    trials_csp = zeros(size(trials));

    <span class="keyword">for</span> i = 1:ntrials
        trials_csp(:,:,i) = W' * squeeze(trials(:,:,i));
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><pre class="codeinput"><span class="comment">% Apply the functions</span>
W = csp(trials_filt.right, trials_filt.foot);
trials_csp = struct(<span class="string">'right'</span>, apply_mix(W, trials_filt.right), <span class="keyword">...</span>
                    <span class="string">'foot'</span>, apply_mix(W, trials_filt.foot));
</pre><p>To see the result of the CSP algorithm, we plot the log-var like we did before:</p><pre class="codeinput">trials_logvar = struct(<span class="string">'right'</span>, logvar(trials_csp.right), <span class="keyword">...</span>
                       <span class="string">'foot'</span>, logvar(trials_csp.foot));
plot_logvar(trials_logvar);
</pre><img vspace="5" hspace="5" src="tutorial3_04.png" alt=""> <p>Instead of 118 channels, we now have 118 mixtures of channels, called components. They are the result of 118 spatial filters applied to the data.</p><p>The first filters maximize the variation of the first class, while minimizing the variation of the second. The last filters maximize the variation of the second class, while minimizing the variation of the first.</p><p>This is also visible in a PSD plot. The code below plots the PSD for the first and last components as well as one in the middle:</p><pre class="codeinput">[psd_r, freqs] = calc_psd(trials_csp.right);
[psd_f, freqs] = calc_psd(trials_csp.foot);
trials_PSD = struct(<span class="string">'right'</span>, psd_r, <span class="string">'foot'</span>, psd_f);

plot_psds(trials_PSD, freqs, [1, 59, 118], <span class="keyword">...</span>
          {<span class="string">'first component'</span>, <span class="string">'middle component'</span>, <span class="string">'last component'</span>}, 0.75);
</pre><img vspace="5" hspace="5" src="tutorial3_05.png" alt=""> <p>In order to see how well we can differentiate between the two classes, a scatter plot is a useful tool. Here both classes are plotted on a 2-dimensional plane: the x-axis is the first CSP component, the y-axis is the last.</p><pre class="codeinput"><span class="keyword">function</span> plot_scatter(right, foot)
    figure();
    hold <span class="string">on</span>;
    scatter(right(1,:), right(end,:), <span class="string">'b'</span>);
    scatter(foot(1,:), foot(end,:), <span class="string">'r'</span>);
    hold <span class="string">off</span>;
    xlabel(<span class="string">'Last component'</span>);
    ylabel(<span class="string">'First component'</span>);
    title(<span class="string">'Right hand versus foot movement'</span>);
    legend({<span class="string">'right'</span>, <span class="string">'foot'</span>});
<span class="keyword">end</span>
</pre><pre class="codeinput">plot_scatter(trials_logvar.right, trials_logvar.foot)
</pre><img vspace="5" hspace="5" src="tutorial3_06.png" alt=""> <p>We will apply a linear classifier to this data. A linear classifier can be thought of as drawing a line in the above plot to separate the two classes. To determine the class for a new trial, we just check on which side of the line the trial would be if plotted as above.</p><p>The data is split into a train and a test set. The classifier will fit a model (in this case, a straight line) on the training set and use this model to make predictions about the test set (see on which side of the line each trial in the test set falls). Note that the CSP algorithm is part of the model, so for fairness sake it should be calculated using only the training data.</p><pre class="codeinput"><span class="comment">%Percentage of trials to use for training (50-50 split here)</span>
train_percentage = 0.5;

<span class="comment">% Calculate the number of trials for each class the above percentage boils</span>
<span class="comment">% down to</span>
ntrain_r = fix(size(trials_filt.right, 3) * train_percentage);
ntrain_f = fix(size(trials_filt.foot, 3) * train_percentage);
ntest_r = size(trials_filt.right, 3) - ntrain_r;
ntest_f = size(trials_filt.foot, 3) - ntrain_f;

<span class="comment">% Splitting the frequency filtered signal into a train and test set</span>
train = struct(<span class="string">'right'</span>, trials_filt.right(:,:,1:ntrain_r), <span class="keyword">...</span>
               <span class="string">'foot'</span>, trials_filt.foot(:,:,1:ntrain_f));

test = struct(<span class="string">'right'</span>, trials_filt.right(:,:,ntrain_r+1:end), <span class="keyword">...</span>
              <span class="string">'foot'</span>, trials_filt.foot(:,:,ntrain_f+1:end));

<span class="comment">% Train the CSP on the training set only</span>
W = csp(train.right, train.foot);

<span class="comment">% Apply the CSP on both the training and test set</span>
train.right = apply_mix(W, train.right);
train.foot = apply_mix(W, train.foot);
test.right = apply_mix(W, test.right);
test.foot = apply_mix(W, test.foot);

<span class="comment">% Select only the first and last components for classification</span>
comp = [1,118];
train.right = train.right(comp,:,:);
train.foot = train.foot(comp,:,:);
test.right = test.right(comp,:,:);
test.foot = test.foot(comp,:,:);

<span class="comment">% Calculate the log-var feature</span>
train.right = logvar(train.right);
train.foot = logvar(train.foot);
test.right = logvar(test.right);
test.foot = logvar(test.foot);
</pre><p>For a classifier the Linear Discriminant Analysis (LDA) algorithm will be used. It fits a gaussian distribution to each class, characterized by the mean and covariance, and determines an optimal separating plane to divide the two. This plane is defined as <span class="MathJax_Preview"><img src="tutorial3_eq50211.png" alt="$r = W_1 \cdot X_1 + W_2 \cdot X_2 + \ldots + W_n \cdot X_n - b$"></span><script type="math/tex">r = W_1 \cdot X_1 + W_2 \cdot X_2 + \ldots + W_n \cdot X_n - b</script>, where <span class="MathJax_Preview"><img src="tutorial3_eq25861.png" alt="$r$"></span><script type="math/tex">r</script> is the classifier output, <span class="MathJax_Preview"><img src="tutorial3_eq54257.png" alt="$W$"></span><script type="math/tex">W</script> are called the feature weights, <span class="MathJax_Preview"><img src="tutorial3_eq03598.png" alt="$X$"></span><script type="math/tex">X</script> are the features of the trial, <span class="MathJax_Preview"><img src="tutorial3_eq25947.png" alt="$n$"></span><script type="math/tex">n</script> is the dimensionality of the data and <span class="MathJax_Preview"><img src="tutorial3_eq28812.png" alt="$b$"></span><script type="math/tex">b</script> is called the offset.</p><p>In our case we have 2 dimensional data, so the separating plane will be a line: <span class="MathJax_Preview"><img src="tutorial3_eq04408.png" alt="$r = W_1 \cdot X_1 + W_2 \cdot X_2 - b$"></span><script type="math/tex">r = W_1 \cdot X_1 + W_2 \cdot X_2 - b</script>. To determine a class label for an unseen trial, we can calculate whether the result is positive or negative.</p><pre class="codeinput"><span class="keyword">function</span> [W, b] = train_lda(class1, class2)
<span class="comment">%     Trains the LDA algorithm.</span>
<span class="comment">%     arguments:</span>
<span class="comment">%         class1 - An array (features x trials) for class 1</span>
<span class="comment">%         class2 - An array (features x trails) for class 2)</span>
<span class="comment">%     returns:</span>
<span class="comment">%         The projection matrix W</span>
<span class="comment">%         The offset b</span>
    m1 = mean(class1');
    m2 = mean(class2');

    W = (m2 - m1) / (cov(class1') + cov(class2'));
    b = (m1 + m2) * W' / 2;
<span class="keyword">end</span>

<span class="keyword">function</span> prediction = apply_lda(test, W, b)
<span class="comment">%     Applies a previously trained LDA to new data.</span>
<span class="comment">%     arguments:</span>
<span class="comment">%         test - An array (features x trials) containing the data</span>
<span class="comment">%         W    - The project matrix W as calculated by train_lda()</span>
<span class="comment">%         b    - The offsets b as calculated by train_lda()</span>
<span class="comment">%     returns:</span>
<span class="comment">%         A list containing a classlabel for each trial</span>
    ntrials = size(test, 2);

    prediction = [];
    <span class="keyword">for</span> i = 1:ntrials
        <span class="comment">% The line below is a generalization for:</span>
        <span class="comment">% result = W(0) * test(0,i) + W(1) * test(1,i) - b</span>
        result = W * test(:,i) - b;
        <span class="keyword">if</span> result &lt;= 0
            prediction = [prediction 1];
        <span class="keyword">else</span>
            prediction = [prediction 2];
        <span class="keyword">end</span>
    <span class="keyword">end</span>
<span class="keyword">end</span>
</pre><p>Training the LDA using the training data gives us <span class="MathJax_Preview"><img src="tutorial3_eq54257.png" alt="$W$"></span><script type="math/tex">W</script> and <span class="MathJax_Preview"><img src="tutorial3_eq28812.png" alt="$b$"></span><script type="math/tex">b</script>:</p><pre class="codeinput">[W, b] = train_lda(train.right, train.foot);

disp(<span class="string">'W:'</span>); disp(W);
disp(<span class="string">'b:'</span>); disp(b);
</pre><pre class="codeoutput">W:
    5.6921   -4.0950

b:
   -6.8300

</pre><p>It can be informative to recreate the scatter plot and overlay the decision boundary as determined by the LDA classifier. The decision boundary is the line for which the classifier output is exactly 0. The scatterplot used <span class="MathJax_Preview"><img src="tutorial3_eq94566.png" alt="$X_1$"></span><script type="math/tex">X_1</script> as <span class="MathJax_Preview"><img src="tutorial3_eq43551.png" alt="$x$"></span><script type="math/tex">x</script>-axis and <span class="MathJax_Preview"><img src="tutorial3_eq21514.png" alt="$X_2$"></span><script type="math/tex">X_2</script> as <span class="MathJax_Preview"><img src="tutorial3_eq44020.png" alt="$y$"></span><script type="math/tex">y</script>-axis. To find the function <span class="MathJax_Preview"><img src="tutorial3_eq11076.png" alt="$y = f(x)$"></span><script type="math/tex">y = f(x)</script> describing the decision boundary, we set <span class="MathJax_Preview"><img src="tutorial3_eq25861.png" alt="$r$"></span><script type="math/tex">r</script> to 0 and solve for <span class="MathJax_Preview"><img src="tutorial3_eq44020.png" alt="$y$"></span><script type="math/tex">y</script> in the equation of the separating plane:</p><p>$$\begin{align}
W_1 \cdot x + W_2 \cdot y - b &amp;= 0 \\
W_1 \cdot x + W_2 \cdot y &amp;= b \\
W_2 \cdot y &amp;= b - W_1 \cdot x \\
\\
y &amp;= \frac{b - W_1 \cdot x}{W_2}
\end{align}$$</p><pre class="error">Unable to interpret LaTeX string "$$\begin{align} W_1 \cdot x + W_2 \cdot y - b &amp;= 0 \\ W_1 \cdot x + W_2 \cdot y &amp;= b \\ W_2 \cdot y &amp;= b - W_1 \cdot x \\ \\ y &amp;= \frac{b - W_1 \cdot x}{W_2} \end{align}$$"</pre><p>We first plot the decision boundary with the training data used to calculate it:</p><pre class="codeinput"><span class="comment">% Scatterplot like before</span>
plot_scatter(train.right, train.foot);
title(<span class="string">'Training data'</span>);

<span class="comment">% Calculate decision boundary (x,y)</span>
x = (-5:0.1:1);
y = (b - W(1)*x) / W(2);

<span class="comment">% Plot the decision boundary</span>
hold <span class="string">on</span>;
plot(x, y, <span class="string">'--k'</span>, <span class="string">'LineWidth'</span>, 2);
hold <span class="string">off</span>;
xlim([-5, 1]);
ylim([-2.2, 1]);
</pre><img vspace="5" hspace="5" src="tutorial3_07.png" alt=""> <p>The code below plots the boundary with the test data on which we will apply the classifier. You will see the classifier is going to make some mistakes.</p><pre class="codeinput">plot_scatter(test.right, test.foot);
title(<span class="string">'Test data'</span>);
hold <span class="string">on</span>;
plot(x, y, <span class="string">'--k'</span>, <span class="string">'LineWidth'</span>, 2);
hold <span class="string">off</span>;
xlim([-5, 1]);
ylim([-2.2, 1]);
</pre><img vspace="5" hspace="5" src="tutorial3_08.png" alt=""> <p>Now the LDA is constructed and fitted to the training data. We can now apply it to the test data. The results are presented as a confusion matrix:</p><p>
<style>
   table {
       border-collapse: collapse;
   }
   table td {
       border: 1px solid black;
   }
</style>
<table border="1">
    <tr><td></td><td colspan='2' style="font-weight:bold">True labels &#8594;</td></tr>
    <tr><td style="font-weight:bold">&#8595; Predicted labels</td><td>Right</td><td>Foot</td></tr>
    <tr><td>Right</td><td></td><td></td></tr>
    <tr><td>Foot</td><td></td><td></td></tr>
</table>
</p><pre class="codeinput"><span class="comment">% The number at the diagonal will be trials that were correctly classified,</span>
<span class="comment">% any trials incorrectly classified (either a false positive or false</span>
<span class="comment">% negative) will be in the corners.</span>
<span class="comment">% Print confusion matrix</span>
conf = [sum(apply_lda(test.right, W, b) == 1), sum(apply_lda(test.foot, W, b) == 1); <span class="keyword">...</span>
        sum(apply_lda(test.right, W, b) == 2), sum(apply_lda(test.foot, W, b) == 2)];

disp(<span class="string">'Confusion matrix:'</span>); disp(conf);
fprintf(<span class="string">'\nAccuracy: %.3f\n'</span>, sum(diag(conf)) / sum(sum(conf)));
</pre><pre class="codeoutput">Confusion matrix:
    70     5
     0    65


Accuracy: 0.964
</pre><p>The confusion matrix shows that 5 out of the 70 trials with foot movement were incorrectly classified as right hand movement. All the trials with right hand movement were classified correctly. In total, 96% of the trials were correctly classified, not a bad score!</p><pre class="codeinput"><span class="keyword">end</span>
</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.14<br></p></div><!--
##### SOURCE BEGIN #####
function tutorial3()
%% 3. Imagined movement
% In this tutorial we will look at imagined movement. Our movement is
% controlled in the motor cortex where there is an increased level of mu
% activity (8–12 Hz) when we perform movements. This is accompanied by a
% reduction of this mu activity in specific regions that deal with the limb
% that is currently moving. This decrease is called Event Related
% Desynchronization (ERD). By measuring the amount of mu activity at
% different locations on the motor cortex, we can determine which limb the
% subject is moving. Through mirror neurons, this effect also occurs when
% the subject is not actually moving his limbs, but merely imagining it.
%% Credits
% The CSP code was originally written by Boris Reuderink of the Donders
% Institute for Brain, Cognition and Behavior. It is part of his Python EEG
% toolbox: https://github.com/breuderink/eegtools
%
% Inspiration for this tutorial also came from the excellent code example
% given in the book chapter:
% 
% Arnaud Delorme, Christian Kothe, Andrey Vankov, Nima Bigdely-Shamlo,
% Robert Oostenveld, Thorsten Zander, and Scott Makeig. MATLAB-Based Tools
% for BCI Research, In _(B+H)CI: The Human in Brain-Computer Interfaces and
% the Brain in Human-Computer Interaction._ Desney S. Tan and Anton Nijholt
% (eds.), 2009, 241-259, http://dx.doi.org/10.1007/978-1-84996-272-8
%% Obtaining the data
% The dataset for this tutorial is provided by the fourth BCI competition,
% which you will have to download youself. First, go to http://www.bbci.de/competition/iv/#download
% and fill in your name and email address. An email will be sent to you
% automatically containing a username and password for the download area.
%
% Download Data Set 1, from Berlin, the 100Hz version in MATLAB format:
% http://bbci.de/competition/download/competition_iv/BCICIV_1_mat.zip
% and unzip it in a subdirectory called 'data_set_IV'. This subdirectory
% should be inside the directory in which you've stored the tutorial files.
%
% Also download the true labels of this dataset:
% http://bbci.de/competition/iii/results/berlin_IVa/true_labels_al.mat 
% and store them in the same 'data_set_IV' subdirectory you created
% earlier.
% 
% <http://bbci.de/competition/iv/desc_1.html Description of the data>
%%
% If you've followed the instructions above, the following code should load
% the data:
m = load('data_set_IV/100Hz/data_set_IVa_al.mat');
m2 = load('data_set_IV/true_labels_al.mat');

sample_rate = m.nfo.fs;
EEG = m.cnt';
nchannels = size(EEG, 1);
nsamples = size(EEG, 2);

channel_names = m.nfo.clab;
event_onsets = m.mrk.pos;
event_codes = m2.true_y;
cl_lab = m.mrk.className;
nclasses = length(cl_lab);
nevents = length(event_onsets);
%%
% Now we have the data in the following variables:

% Print some information
disp('Shape of EEG:'); disp(size(EEG));
disp('Sample rate:'); disp(sample_rate);
disp('Number of channels:'); disp(nchannels);
disp('Channel names:'); disp(channel_names);
disp('Number of events:'); disp(length(event_onsets));
disp('Event codes:'); disp(unique(event_codes));
disp('Class labels:'); disp(cl_lab);
disp('Number of classes:'); disp(nclasses);
%%
% This is a large recording: 118 electrodes where used, spread across the
% entire scalp. The subject was given a cue and then imagined either right
% hand movement or the movement of his feet. As can be seen from the
% <http://en.wikipedia.org/wiki/Cortical_homunculus Homunculus>, foot
% movement is controlled at the center of the motor cortex (which makes it
% hard to distinguish left from right foot), while hand movement is
% controlled more lateral.
% 
% The code below cuts trials for the two classes and should look familiar
% if you've completed the previous tutorials. Trials are cut in the
% interval [0.5–2.5 s] after the onset of the cue.

% Struct to store the trials in, each class gets an entry
trials = struct();

% The time window (in samples) to extract for each trial, here 0.5 REPLACE_WITH_DASH_DASH 2.5
% seconds
win = fix(0.5*sample_rate):fix(2.5*sample_rate)-1;

% Length of the time window
nsamples = length(win);

% Loop over the classes (right, foot)
codes = unique(event_codes);
for i = 1:length(cl_lab)
    cl = cl_lab{i};
    code = codes(i);
    
    % Extract the onsets for the class
    cl_onsets = event_onsets(event_codes == code);
    
    % Allocate memory for the trials
    trials.(cl) = zeros(nchannels, nsamples, length(cl_onsets));
    
    % Extract each trial
    for j = 1:length(cl_onsets)
        onset = cl_onsets(j);
        trials.(cl)(:,:,j) = EEG(:, win+onset);
    end
end
  
% Some information about the dimensionality of the data (channels x time x trials)
disp('Shape of trials.right:'); disp(size(trials.right));
disp('Shape of trials.foot:'); disp(size(trials.foot));

%% Plotting the data
% Since the feature we're looking for (a decrease in mu activity) is a
% frequency feature, lets plot the PSD of the trials in a similar manner as
% with the SSVEP data. The code below defines a function that computes the
% PSD for each trial (we're going to need it again later on):
function [trials_PSD, freqs] = calc_psd(trials)
%     Calculates for each trial the Power Spectral Density (PSD).
%     arguments:
%         trials - An array (channels x samples x trials) containing the signal.
%     returns:
%         An array (channels x PSD x trials) containing the PSD for each trial.
%         A list containing the frequencies for which the PSD was computed (useful for plotting later)
    
    ntrials = size(trials, 3);
    trials_PSD = zeros(nchannels, 101, ntrials);

    % Iterate over trials and channels
    for trial = 1:ntrials
        for ch = 1:nchannels
            % Calculate the PSD
            [PSD, freqs] = pwelch(squeeze(trials(ch,:,trial)), nsamples, 0, nsamples, sample_rate);
            trials_PSD(ch, :, trial) = PSD;
        end
    end
end
%%

% Apply the function
[psd_r, freqs] = calc_psd(trials.right);
[psd_f, freqs] = calc_psd(trials.foot);
trials_PSD = struct('right', psd_r, 'foot', psd_f);
%%
% The function below plots the PSDs that are calculated with the above
% function. Since plotting it for 118 channels will clutter the display,
% it takes the indices of the desired channels as input, as well as some
% metadata to decorate the plot.

function plot_psds(trials_PSD, freqs, chan_ind, chan_lab, maxy)
%     Plots PSD data calculated with psd().
%     arguments:
%         trials   - The PSD data, as returned by psd()
%         freqs    - The frequencies for which the PSD is defined, as returned by psd() 
%         chan_ind - List of indices of the channels to plot
%         chan_lab - (optional) List of names for each channel
%         maxy     - (optional) Limit the y-axis to this value
    figure('Position', [100, 100, 1000, 300]);
    
    nchans = length(chan_ind);
    
    % Maximum of 3 plots per row
    nrows = ceil(nchans / 3);
    ncols = min(3, nchans);
    
    % Enumerate over the channels
    for i = 1:length(chan_ind)
        ch = chan_ind(i);
        
        % Figure out which subplot to draw to
        subplot(nrows,ncols,i);
    
        % Plot the PSD for each class
        hold on;
        colors = {'b', 'r'};
        for j = 1:nclasses
            cl = cl_lab{j};
            plot(freqs, squeeze(mean(trials_PSD.(cl)(ch,:,:), 3)), colors{j});
        end
        hold off;
        
        % All plot decoration below...
        xlim([1, 30]);
        ylim([0, maxy]);
    
        grid()
        
        xlabel('Frequency (Hz)')
        title(chan_lab{i});
        legend(cl_lab);
    end
end
%%
% Lets put the `plot_psds()` function to use and plot three channels:
% 
% # C3: Central, left
% # Cz: Central, central
% # C4: Central, right
channels_of_interest = [find(strcmp(channel_names, 'C3')), ...
                        find(strcmp(channel_names, 'Cz')), ...
                        find(strcmp(channel_names, 'C4'))];
plot_psds(trials_PSD, freqs, channels_of_interest, ...
         {'left', 'center', 'right'}, 2500);
%%
% A spike of mu activity can be seen on each channel for both classes. At
% the left hemisphere, the mu for the right hand movement is lower than for
% the foot movement due to the ERD. At the central electrode, the mu for
% the foot movement is reduced and at the right electrode the mu activity
% is about equal for both classes. This is in line with the theory that the
% right hand is controlled by the left hemiphere and the feet are
% controlled centrally.
%% Classifying the data
% We will use a machine learning algorithm to construct a model that can
% distinguish between the right hand and foot movement of this subject. In
% order to do this we need to:
% 
% # find a way to quantify the amount of mu activity present in a trial
% # make a model that describes expected values of mu activity for each class
% # finally test this model on some unseen data to see if it can predict the correct class label
% 
% We will follow a classic BCI design by Blankertz et al. [1] where they
% use the logarithm of the variance of the signal in a certain frequency
% band as a feature for the classifier.
% 
% [1] Blankertz, B., Dornhege, G., Krauledat, M., Müller, K.-R., & Curio,
% G. (2007). The non-invasive Berlin Brain-Computer Interface: fast
% acquisition of effective performance in untrained subjects. _NeuroImage_,
% 37(2), 539–550. doi:10.1016/j.neuroimage.2007.01.051
% 
% The script below designs a band pass filter using the |butter| function
% that will strip away frequencies outside the 8REPLACE_WITH_DASH_DASH15Hz window. The filter
% is applied to all trials:
function trials_filt = bandpass(trials, lo, hi, sample_rate)
%     Designs and applies a bandpass filter to the signal.
%     arguments:
%         trials      - An array (channels x samples x trials) containing the signal
%         lo          - Lower frequency bound (in Hz)
%         hi          - Upper frequency bound (in Hz)
%         sample_rate - Sample rate of the signal (in Hz)
%     returns:
%         An array (channels x samples x trials) containing the bandpassed
%         signal
    ntrials = size(trials, 3);
    
    % The butter() function takes the filter order: higher numbers mean a
    % sharper frequency cutoff, but the resulting signal might be shifted
    % in time, lower numbers mean a soft frequency cutoff, but the
    % resulting signal less distorted in time. It also takes the lower and
    % upper frequency bounds to pass, divided by the niquist frequency,
    % which is the sample rate divided by 2:
    [a, b] = butter(3, [lo/(sample_rate/2.0), hi/(sample_rate/2.0)]);

    % Applying the filter to each trial.
    % filtfilt operates across the first non-singleton dimension, so we
    % permute the trials array so that the samples are on the first
    % dimension
    trials = permute(trials, [2, 1, 3]);
    
    trials_filt = zeros(size(trials));
    for i = 1:ntrials
        trials_filt(:,:,i) = filtfilt(a, b, trials(:,:,i));
    end
    
    % Undo the permutation performed earlier
    trials_filt = permute(trials_filt, [2, 1, 3]);
end
%%
% Apply the function:
trials_filt = struct('right', bandpass(trials.right, 8, 15, sample_rate), ...
                     'foot', bandpass(trials.foot, 8, 15, sample_rate));
%%
% Plotting the PSD of the resulting trials_filt shows the suppression of
% frequencies outside the passband of the filter:
[psd_r, freqs] = calc_psd(trials_filt.right);
[psd_f, freqs] = calc_psd(trials_filt.foot);
trials_PSD = struct('right', psd_r, 'foot', psd_f);

plot_psds(trials_PSD, freqs, channels_of_interest, ...
          {'left', 'center', 'right'}, 3000);
%%
% As a feature for the classifier, we will use the logarithm of the
% variance of each channel. The function below calculates this:
function x = logvar(trials)
%     Calculate the log-var of each channel.
%     arguments:
%         trials - An array (channels x samples x trials) containing the signals.
%     returns:
%         An array (channels x trials) containing for each channel the logvar of the signal

    % var operates along the first non singleton dimension, permute the
    % array to align the samples along the first dimension
    trials = permute(trials, [2, 1, 3]);
    
    % Calculate the log-var
    x = log(squeeze(var(trials)));
end
%%
% Apply the function
trials_logvar = struct('right', logvar(trials_filt.right), ...
                       'foot', logvar(trials_filt.foot));
%%
% Below is a function to visualize the logvar of each channel as a bar
% chart:
function plot_logvar(trials)
%     Plots the log-var of each channel/component.
%     arguments:
%         trials - Dictionary containing the trials (log-vars x trials) for 2 classes.
    figure('Position', [100, 100, 1200, 400]);
   
    x0 = (1:nchannels);
    x1 = (1:nchannels) + 0.4;

    y0 = mean(trials.right, 2);
    y1 = mean(trials.foot, 2);
    
    hold on;
    bar(x0, y0, 0.5, 'b');
    bar(x1, y1, 0.4, 'r');
    hold off;
    
    xlim([-0.5, nchannels+0.5]);

    grid();
    title('log-var of each channel/component');
    xlabel('channels/components');
    ylabel('log-var');
    legend({'right', 'foot'});
end
%%
% Plot the log-vars
plot_logvar(trials_logvar);
%%
% We see that most channels show a small difference in the log-var of the
% signal between the two classes. The next step is to go from 118 channels
% to only a few channel mixtures. The CSP algorithm calculates mixtures of
% channels that are designed to maximize the difference in variation
% between two classes. These mixures are called spatial filters.
function W = csp(trials_r, trials_f)
%     Calculate the CSP transformation matrix W.
%     arguments:
%         trials_r - Array (channels x samples x trials) containing right hand movement trials
%         trials_f - Array (channels x samples x trials) containing foot movement trials
%     returns:
%         Mixing matrix W
    cov_r = trial_cov(trials_r);
    cov_f = trial_cov(trials_f);
    P = whitening(cov_r + cov_f);
    [B,~,~] = svd(P' * cov_f * P);
    W = P * B;
end

function X = trial_cov(trials)
    % Calculate the covariance for each trial and return their average
    [nchannels, nsamples, ntrials] = size(trials);
    covs = zeros(nchannels, nchannels, ntrials);
    for i = 1:ntrials
        covs(:,:,i) = (squeeze(trials(:,:,i)) * squeeze(trials(:,:,i))') / nsamples;
    end

    X = mean(covs, 3);
end

function X = whitening(sigma)
    % Calculate a whitening matrix for covariance matrix sigma.
    [U, l, ~] = svd(sigma);
    X = U * (l ^ -0.5);
end

function trials_csp = apply_mix(W, trials)
    % Apply a mixing matrix to each trial (basically multiply W with the
    % EEG signal matrix)
    ntrials = size(trials, 3);
    trials_csp = zeros(size(trials));
    
    for i = 1:ntrials
        trials_csp(:,:,i) = W' * squeeze(trials(:,:,i));
    end
end
%%

% Apply the functions
W = csp(trials_filt.right, trials_filt.foot);
trials_csp = struct('right', apply_mix(W, trials_filt.right), ...
                    'foot', apply_mix(W, trials_filt.foot));
%%
% To see the result of the CSP algorithm, we plot the log-var like we did
% before:
trials_logvar = struct('right', logvar(trials_csp.right), ...
                       'foot', logvar(trials_csp.foot));
plot_logvar(trials_logvar);
%%
% Instead of 118 channels, we now have 118 mixtures of channels, called
% components. They are the result of 118 spatial filters applied to the
% data.
%
% The first filters maximize the variation of the first class, while
% minimizing the variation of the second. The last filters maximize the
% variation of the second class, while minimizing the variation of the
% first.
%
% This is also visible in a PSD plot. The code below plots the PSD for the
% first and last components as well as one in the middle:
[psd_r, freqs] = calc_psd(trials_csp.right);
[psd_f, freqs] = calc_psd(trials_csp.foot);
trials_PSD = struct('right', psd_r, 'foot', psd_f);

plot_psds(trials_PSD, freqs, [1, 59, 118], ...
          {'first component', 'middle component', 'last component'}, 0.75);
%%
% In order to see how well we can differentiate between the two classes, a
% scatter plot is a useful tool. Here both classes are plotted on a
% 2-dimensional plane: the x-axis is the first CSP component, the y-axis is
% the last.
function plot_scatter(right, foot)
    figure();
    hold on;
    scatter(right(1,:), right(end,:), 'b');
    scatter(foot(1,:), foot(end,:), 'r');
    hold off;
    xlabel('Last component');
    ylabel('First component');
    title('Right hand versus foot movement');
    legend({'right', 'foot'});
end

%%
plot_scatter(trials_logvar.right, trials_logvar.foot)
%%
% We will apply a linear classifier to this data. A linear classifier can
% be thought of as drawing a line in the above plot to separate the two
% classes. To determine the class for a new trial, we just check on which
% side of the line the trial would be if plotted as above.
%
% The data is split into a train and a test set. The classifier will fit a
% model (in this case, a straight line) on the training set and use this
% model to make predictions about the test set (see on which side of the
% line each trial in the test set falls). Note that the CSP algorithm is
% part of the model, so for fairness sake it should be calculated using
% only the training data.

%Percentage of trials to use for training (50-50 split here)
train_percentage = 0.5;

% Calculate the number of trials for each class the above percentage boils
% down to
ntrain_r = fix(size(trials_filt.right, 3) * train_percentage);
ntrain_f = fix(size(trials_filt.foot, 3) * train_percentage);
ntest_r = size(trials_filt.right, 3) - ntrain_r;
ntest_f = size(trials_filt.foot, 3) - ntrain_f;

% Splitting the frequency filtered signal into a train and test set
train = struct('right', trials_filt.right(:,:,1:ntrain_r), ...
               'foot', trials_filt.foot(:,:,1:ntrain_f));

test = struct('right', trials_filt.right(:,:,ntrain_r+1:end), ...
              'foot', trials_filt.foot(:,:,ntrain_f+1:end));

% Train the CSP on the training set only
W = csp(train.right, train.foot);

% Apply the CSP on both the training and test set
train.right = apply_mix(W, train.right);
train.foot = apply_mix(W, train.foot);
test.right = apply_mix(W, test.right);
test.foot = apply_mix(W, test.foot);

% Select only the first and last components for classification
comp = [1,118];
train.right = train.right(comp,:,:);
train.foot = train.foot(comp,:,:);
test.right = test.right(comp,:,:);
test.foot = test.foot(comp,:,:);

% Calculate the log-var feature
train.right = logvar(train.right);
train.foot = logvar(train.foot);
test.right = logvar(test.right);
test.foot = logvar(test.foot);
%%
% For a classifier the Linear Discriminant Analysis (LDA) algorithm will be
% used. It fits a gaussian distribution to each class, characterized by the
% mean and covariance, and determines an optimal separating plane to divide
% the two. This plane is defined as $r = W_1 \cdot X_1 + W_2 \cdot X_2 +
% \ldots + W_n \cdot X_n - b$, where $r$ is the classifier output, $W$ are
% called the feature weights, $X$ are the features of the trial, $n$ is the
% dimensionality of the data and $b$ is called the offset.
% 
% In our case we have 2 dimensional data, so the separating plane will be a
% line: $r = W_1 \cdot X_1 + W_2 \cdot X_2 - b$. To determine a class label
% for an unseen trial, we can calculate whether the result is positive or
% negative.
function [W, b] = train_lda(class1, class2)
%     Trains the LDA algorithm.
%     arguments:
%         class1 - An array (features x trials) for class 1
%         class2 - An array (features x trails) for class 2)
%     returns:
%         The projection matrix W
%         The offset b
    m1 = mean(class1');
    m2 = mean(class2');

    W = (m2 - m1) / (cov(class1') + cov(class2'));
    b = (m1 + m2) * W' / 2;
end

function prediction = apply_lda(test, W, b)
%     Applies a previously trained LDA to new data.
%     arguments:
%         test - An array (features x trials) containing the data
%         W    - The project matrix W as calculated by train_lda()
%         b    - The offsets b as calculated by train_lda()
%     returns:
%         A list containing a classlabel for each trial
    ntrials = size(test, 2);
    
    prediction = [];
    for i = 1:ntrials
        % The line below is a generalization for:
        % result = W(0) * test(0,i) + W(1) * test(1,i) - b
        result = W * test(:,i) - b;
        if result <= 0
            prediction = [prediction 1];
        else
            prediction = [prediction 2];
        end
    end
end
%%
% Training the LDA using the training data gives us $W$ and $b$:
[W, b] = train_lda(train.right, train.foot);

disp('W:'); disp(W);
disp('b:'); disp(b);
%%
% It can be informative to recreate the scatter plot and overlay the
% decision boundary as determined by the LDA classifier. The decision
% boundary is the line for which the classifier output is exactly 0. The
% scatterplot used $X_1$ as $x$-axis and $X_2$ as $y$-axis. To find the
% function $y = f(x)$ describing the decision boundary, we set $r$ to 0 and
% solve for $y$ in the equation of the separating plane:
%%
% $$\begin{align}
% W_1 \cdot x + W_2 \cdot y - b &= 0 \\
% W_1 \cdot x + W_2 \cdot y &= b \\
% W_2 \cdot y &= b - W_1 \cdot x \\
% \\
% y &= \frac{b - W_1 \cdot x}{W_2}
% \end{align}$$
%%
% We first plot the decision boundary with the training data used to
% calculate it:

% Scatterplot like before
plot_scatter(train.right, train.foot);
title('Training data');

% Calculate decision boundary (x,y)
x = (-5:0.1:1);
y = (b - W(1)*x) / W(2);

% Plot the decision boundary
hold on;
plot(x, y, 'REPLACE_WITH_DASH_DASHk', 'LineWidth', 2);
hold off;
xlim([-5, 1]);
ylim([-2.2, 1]);
%%
% The code below plots the boundary with the test data on which we will
% apply the classifier. You will see the classifier is going to make some
% mistakes.
plot_scatter(test.right, test.foot);
title('Test data');
hold on;
plot(x, y, 'REPLACE_WITH_DASH_DASHk', 'LineWidth', 2);
hold off;
xlim([-5, 1]);
ylim([-2.2, 1]);
%%
% Now the LDA is constructed and fitted to the training data. We can now
% apply it to the test data. The results are presented as a confusion
% matrix:
%     
% 
% <html>
% <style>
%    table {
%        border-collapse: collapse;
%    }
%    table td {
%        border: 1px solid black;
%    }
% </style>
% <table border="1">
%     <tr><td></td><td colspan='2' style="font-weight:bold">True labels &#8594;</td></tr>
%     <tr><td style="font-weight:bold">&#8595; Predicted labels</td><td>Right</td><td>Foot</td></tr>
%     <tr><td>Right</td><td></td><td></td></tr>
%     <tr><td>Foot</td><td></td><td></td></tr>
% </table>
% </html>

% The number at the diagonal will be trials that were correctly classified,
% any trials incorrectly classified (either a false positive or false
% negative) will be in the corners.
% Print confusion matrix
conf = [sum(apply_lda(test.right, W, b) == 1), sum(apply_lda(test.foot, W, b) == 1); ...
        sum(apply_lda(test.right, W, b) == 2), sum(apply_lda(test.foot, W, b) == 2)];

disp('Confusion matrix:'); disp(conf);
fprintf('\nAccuracy: %.3f\n', sum(diag(conf)) / sum(sum(conf)));
%%
% The confusion matrix shows that 5 out of the 70 trials with foot movement
% were incorrectly classified as right hand movement. All the trials with
% right hand movement were classified correctly. In total, 96% of the
% trials were correctly classified, not a bad score!
end
##### SOURCE END #####
--></body></html>